import streamlit as st
import os
from utils.embedding_utils import EmbeddingModel
from utils.vector_store import VectorStore
from utils.llm_utils import LLMHandler
from utils.schema import Question, Response, Document
from utils.document_processor import DocumentProcessor
import json
import numpy as np

# Load custom CSS
def load_css():
    with open("assets/style.css") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
        
# Initialize session state
def init_session_state():
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'vector_store' not in st.session_state:
        try:
            model_path = os.environ.get("BGE_MODEL_PATH", "BAAI/bge-m3")
            st.session_state.embedding_model = EmbeddingModel(model_path)
            st.session_state.vector_store = VectorStore(
                dimension=st.session_state.embedding_model.get_embedding_dimension()
            )
            st.session_state.document_processor = DocumentProcessor()
            st.session_state.processed_files = {}  # è·Ÿè¸ªå·²å¤„ç†çš„æ–‡ä»¶
            #print('åŠ è½½å‘é‡æ¨¡å‹æˆåŠŸ')
        except Exception as e:
            st.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
            st.session_state.embedding_model = None
            st.session_state.vector_store = None
            st.session_state.document_processor = None
            
    if 'llm_handler' not in st.session_state:
        try:
            model_path = os.environ.get("LLM_MODEL_PATH", "/media/mynewdrive/llm_store/model_store/qwen2-7b-instruct")
            #print(model_path)
            if not os.path.exists(model_path):
                st.warning(f"æ¨¡å‹è·¯å¾„æœªæ‰¾åˆ°ï¼š{model_path}")
                st.info("è¯·è®¾ç½® LLM_MODEL_PATH ç¯å¢ƒå˜é‡æŒ‡å‘æ‚¨çš„ Qwen æ¨¡å‹ã€‚")
                st.session_state.llm_handler = None
            else:
                st.session_state.llm_handler = LLMHandler(model_path)
        except Exception as e:
            st.error(f"åˆå§‹åŒ– LLM æ—¶å‡ºé”™ï¼š{str(e)}")
            st.session_state.llm_handler = None

def display_chat_history():
    """Display the chat history."""
    for message in st.session_state.chat_history:
        with st.container():
            if message["role"] == "user":
                st.markdown(f"**æ‚¨ï¼š** {message['content']}")
            else:
                st.markdown(f"**åŠ©æ‰‹ï¼š** {message['content']}")
                if message.get("sources"):
                    with st.expander("å‚è€ƒæ¥æº"):
                        for source in message["sources"]:
                            st.write(source)

def process_question(question: str) -> Response:
    """Process a user question and generate a response."""
    try:
        if not st.session_state.llm_handler:
            return Response(
                answer="æŠ±æ­‰ï¼Œè¯­è¨€æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–ã€‚è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„è®¾ç½®æ­£ç¡®ã€‚",
                confidence=0.0
            )
        question_embedding = st.session_state.embedding_model.generate_embeddings(question)
        search_results = st.session_state.vector_store.search(
            question_embedding,
            k=10
        )
        context = []
        for doc, score in search_results:
            context.append(f"[æ¥æºï¼š{doc.metadata['filename']}, ç‰‡æ®µï¼š{doc.metadata['chunk_id'] + 1}/{doc.metadata['total_chunks']}]\n{doc.content}")
        
        recent_history = st.session_state.chat_history[-8:] if len(st.session_state.chat_history) > 0 else None
        response = st.session_state.llm_handler.generate_response(
            question=question,
            context=context,
            chat_history=recent_history
        )
        return response
        
    except Exception as e:
        st.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™ï¼š{str(e)}")
        return Response(
            answer="æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°é”™è¯¯ã€‚è¯·é‡è¯•ã€‚",
            confidence=0.0
        )

def process_uploaded_file(uploaded_file):
    """Process an uploaded file and add it to the store."""
    try:
        if uploaded_file.name in st.session_state.processed_files:
            st.info(f"æ–‡ä»¶ {uploaded_file.name} å·²ç»è¢«å¤„ç†è¿‡ã€‚")
            return st.session_state.processed_files[uploaded_file.name]
        
        documents = st.session_state.document_processor.process_file(
            uploaded_file,
            uploaded_file.name
        )
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, doc in enumerate(documents):
            progress = (i + 1) / len(documents)
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(documents)} ä¸ªç‰‡æ®µ...")
            
            embeddings = st.session_state.embedding_model.generate_embeddings(doc.content)
            st.session_state.vector_store.add_documents([doc], embeddings.reshape(1, -1))
        
        progress_bar.empty()
        status_text.empty()
        
        st.session_state.processed_files[uploaded_file.name] = len(documents)
        return len(documents)
        
    except Exception as e:
        raise Exception(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")

def send():
    """Callback function to handle sending the question."""
    if st.session_state.question:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
        st.session_state.chat_history.append({
            "role": "user",
            "content": st.session_state.question
        })
        
        # å¤„ç†é—®é¢˜å¹¶è·å–å›å¤
        response = process_question(st.session_state.question)
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°èŠå¤©å†å²
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": response.answer,
            "sources": response.sources,
            "confidence": response.confidence,
            "tool_calls": response.tool_calls
        })
        
        # æ¸…ç©ºè¾“å…¥æ¡†
        st.session_state.question = ""
    else:
        st.warning("è¯·è¾“å…¥é—®é¢˜ã€‚")

def main():
    st.set_page_config(
        page_title="IDICC-AIR",    # ç½‘é¡µæ ‡ç­¾é¡µçš„æ ‡é¢˜
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    load_css()
    init_session_state()
    st.title("AIR, ä½ çš„äº§ä¸šæƒ…æŠ¥åŠ©æ‰‹")
    
    # Sidebar for file upload and settings
    with st.sidebar:
        st.header("è®¾ç½®")
        
        # File upload for document ingestion
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£",
            type=["txt", "pdf", "docx"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                if uploaded_file.name not in st.session_state.processed_files:
                    try:
                        with st.spinner(f"æ­£åœ¨å¤„ç† {uploaded_file.name}..."):
                            num_chunks = process_uploaded_file(uploaded_file)
                            st.success(f"æˆåŠŸå°† {uploaded_file.name} å¤„ç†ä¸º {num_chunks} ä¸ªç‰‡æ®µï¼")
                            
                    except Exception as e:
                        st.error(f"å¤„ç† {uploaded_file.name} æ—¶å‡ºé”™ï¼š{str(e)}")
                else:
                    st.info(f"æ–‡ä»¶ {uploaded_file.name} å·²ç»è¢«å¤„ç†è¿‡ã€‚")
    
    # Main chat interface
    display_chat_history()
    
    # Question input with key and callback
    st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key='question')
    
    st.button("å‘é€", on_click=send)
    

if __name__ == "__main__":
    main()