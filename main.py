import streamlit as st
import os
from utils.embedding_utils import EmbeddingModel
from utils.vector_store import VectorStore
from utils.llm_utils import LLMHandler
from utils.schema import Question, Response, Document
from utils.document_processor import DocumentProcessor
import json
import numpy as np

# Load custom CSS
def load_css():
    with open("assets/style.css") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
        
# Initialize session state
def init_session_state():
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'vector_store' not in st.session_state:
        try:
            model_path = os.environ.get("BGE_MODEL_PATH", "BAAI/bge-m3")
            st.session_state.embedding_model = EmbeddingModel(model_path)
            st.session_state.vector_store = VectorStore(
                dimension=st.session_state.embedding_model.get_embedding_dimension()
            )
            st.session_state.document_processor = DocumentProcessor()
        except Exception as e:
            st.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
            st.session_state.embedding_model = None
            st.session_state.vector_store = None
            st.session_state.document_processor = None
            
    if 'llm_handler' not in st.session_state:
        try:
            model_path = os.environ.get("LLM_MODEL_PATH", "/media/mynewdrive/llm_store/model_store/qwen2-7b-instruct")
            if not os.path.exists(model_path):
                st.warning(f"æ¨¡å‹è·¯å¾„æœªæ‰¾åˆ°ï¼š{model_path}")
                st.info("è¯·è®¾ç½® LLM_MODEL_PATH ç¯å¢ƒå˜é‡æŒ‡å‘æ‚¨çš„ Qwen æ¨¡å‹ã€‚")
                st.session_state.llm_handler = None
            else:
                st.session_state.llm_handler = LLMHandler(model_path)
        except Exception as e:
            st.error(f"åˆå§‹åŒ– LLM æ—¶å‡ºé”™ï¼š{str(e)}")
            st.session_state.llm_handler = None

def display_chat_history():
    """Display the chat history."""
    for message in st.session_state.chat_history:
        with st.container():
            if message["role"] == "user":
                st.markdown(f"**æ‚¨ï¼š** {message['content']}")
            else:
                st.markdown(f"**åŠ©æ‰‹ï¼š** {message['content']}")
                if message.get("sources"):
                    with st.expander("å‚è€ƒæ¥æº"):
                        for source in message["sources"]:
                            st.write(source)

def process_question(question: str):
    """Process a user question and generate a response."""
    try:
        if not st.session_state.llm_handler:
            return Response(
                answer="æŠ±æ­‰ï¼Œè¯­è¨€æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–ã€‚è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„è®¾ç½®æ­£ç¡®ã€‚",
                confidence=0.0
            )
        
        question_embedding = st.session_state.embedding_model.generate_embeddings(question)
        search_results = st.session_state.vector_store.search(
            question_embedding,
            k=3
        )
        
        context = []
        for doc, score in search_results:
            context.append(f"[æ¥æºï¼š{doc.metadata['filename']}, ç‰‡æ®µï¼š{doc.metadata['chunk_id'] + 1}/{doc.metadata['total_chunks']}]\n{doc.content}")
        
        recent_history = st.session_state.chat_history[-8:] if len(st.session_state.chat_history) > 0 else None
        
        response_placeholder = st.empty()
        response_text = ""
        
        for partial_response in st.session_state.llm_handler.generate_response_stream(
            question=question,
            context=context,
            chat_history=recent_history
        ):
            response_text += partial_response
            response_placeholder.markdown(f"**åŠ©æ‰‹ï¼š** {response_text}")
        
        return response_text
        
    except Exception as e:
        st.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™ï¼š{str(e)}")
        return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°é”™è¯¯ã€‚è¯·é‡è¯•ã€‚"

def process_uploaded_file(uploaded_file):
    """Process an uploaded file and add it to the vector store."""
    try:
        documents = st.session_state.document_processor.process_file(
            uploaded_file,
            uploaded_file.name
        )
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, doc in enumerate(documents):
            progress = (i + 1) / len(documents)
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(documents)} ä¸ªç‰‡æ®µ...")
            
            embeddings = st.session_state.embedding_model.generate_embeddings(doc.content)
            st.session_state.vector_store.add_documents([doc], embeddings.reshape(1, -1))
        
        progress_bar.empty()
        status_text.empty()
        
        return len(documents)
        
    except Exception as e:
        raise Exception(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")

def main():
    st.set_page_config(
        page_title="AIR",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    load_css()
    init_session_state()
    st.title("AIR, ä½ çš„äº§ä¸šæƒ…æŠ¥åŠ©æ‰‹")
    
    with st.sidebar:
        st.header("è®¾ç½®")
        
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£",
            type=["txt", "pdf", "docx"],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            for uploaded_file in uploaded_files:
                try:
                    with st.spinner(f"æ­£åœ¨å¤„ç† {uploaded_file.name}..."):
                        num_chunks = process_uploaded_file(uploaded_file)
                        st.success(f"æˆåŠŸå°† {uploaded_file.name} å¤„ç†ä¸º {num_chunks} ä¸ªç‰‡æ®µï¼")
                        
                except Exception as e:
                    st.error(f"å¤„ç† {uploaded_file.name} æ—¶å‡ºé”™ï¼š{str(e)}")
    
    display_chat_history()
    
    question = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
    
    if st.button("å‘é€"):
        if question:
            st.session_state.chat_history.append({
                "role": "user",
                "content": question
            })
            
            response = process_question(question)
            
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": response
            })
            
            st.session_state.question = ""
            st.rerun()
        else:
            st.warning("è¯·è¾“å…¥é—®é¢˜ã€‚")

if __name__ == "__main__":
    main()
